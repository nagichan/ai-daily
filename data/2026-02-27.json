{
  "date": "2026-02-27",
  "aiNews": [],
  "papers": [
    {
      "id": "http://arxiv.org/abs/2602.22039v1",
      "title": "TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition",
      "authors": [
        "Cheng-Yeh Yang",
        "Chien-Chun Wang",
        "Li-Wei Chen"
      ],
      "summary": "Low-resource automatic speech recognition (ASR) continues to pose significant challenges, primarily due to the limited availability of transcribed data for numerous languages. While a wealth of spoken content is accessible in television dramas and online videos, Taiwanese Hokkien exemplifies this issue, with transcriptions often being scarce and the majority of available subtitles provided only in Mandarin. To address this deficiency, we introduce TG-ASR for Taiwanese Hokkien drama speech recogn",
      "published": "2026-02-25T15:47:34Z",
      "category": "cs.SD",
      "link": "http://arxiv.org/abs/2602.22039v1",
      "pdf": "http://arxiv.org/pdf/2602.22039v1.pdf"
    },
    {
      "id": "http://arxiv.org/abs/2602.21900v1",
      "title": "EmoOmni: Bridging Emotional Understanding and Expression in Omni-Modal LLMs",
      "authors": [
        "Wenjie Tian",
        "Zhixian Zhao",
        "Jingbin Hu"
      ],
      "summary": "The evolution of Omni-Modal Large Language Models~(Omni-LLMs) has revolutionized human--computer interaction, enabling unified audio-visual perception and speech response. However, existing Omni-LLMs struggle with complex real-world scenarios, often leading to superficial understanding and contextually mismatched emotional responses. This issue is further intensified by Omni-LLM's Thinker-Talker architectures, which are implicitly connected through hidden states, leading to the loss of emotional",
      "published": "2026-02-25T13:30:27Z",
      "category": "cs.SD",
      "link": "http://arxiv.org/abs/2602.21900v1",
      "pdf": "http://arxiv.org/pdf/2602.21900v1.pdf"
    },
    {
      "id": "http://arxiv.org/abs/2602.21476v1",
      "title": "A Knowledge-Driven Approach to Music Segmentation, Music Source Separation and Cinematic Audio Source Separation",
      "authors": [
        "Chun-wei Ho",
        "Sabato Marco Siniscalchi",
        "Kai Li"
      ],
      "summary": "We propose a knowledge-driven, model-based approach to segmenting audio into single-category and mixed-category chunks with applications to source separation. \"Knowledge\" here denotes information associated with the data, such as music scores. \"Model\" here refers to tool that can be used for audio segmentation and recognition, such as hidden Markov models. In contrast to conventional learning that often relies on annotated data with given segment categories and their corresponding boundaries to ",
      "published": "2026-02-25T01:07:42Z",
      "category": "eess.AS",
      "link": "http://arxiv.org/abs/2602.21476v1",
      "pdf": "http://arxiv.org/pdf/2602.21476v1.pdf"
    },
    {
      "id": "http://arxiv.org/abs/2602.21464v1",
      "title": "iMiGUE-Speech: A Spontaneous Speech Dataset for Affective Analysis",
      "authors": [
        "Sofoklis Kakouros",
        "Fang Kang",
        "Haoyu Chen"
      ],
      "summary": "This work presents iMiGUE-Speech, an extension of the iMiGUE dataset that provides a spontaneous affective corpus for studying emotional and affective states. The new release focuses on speech and enriches the original dataset with additional metadata, including speech transcripts, speaker-role separation between interviewer and interviewee, and word-level forced alignments. Unlike existing emotional speech datasets that rely on acted or laboratory-elicited emotions, iMiGUE-Speech captures spont",
      "published": "2026-02-25T00:38:19Z",
      "category": "eess.AS",
      "link": "http://arxiv.org/abs/2602.21464v1",
      "pdf": "http://arxiv.org/pdf/2602.21464v1.pdf"
    },
    {
      "id": "http://arxiv.org/abs/2602.21772v1",
      "title": "UniWhisper: Efficient Continual Multi-task Training for Robust Universal Audio Representation",
      "authors": [
        "Yuxuan Chen",
        "Peize He",
        "Haoyuan Xu"
      ],
      "summary": "A universal audio representation should capture fine-grained speech cues and high-level semantics for environmental sounds and music in a single encoder. Existing encoders often excel in one domain but degrade in others. We propose UniWhisper, an efficient continual multi-task training framework that casts heterogeneous audio tasks into a unified instruction and answer format. This enables standard next-token training without task-specific heads and losses. We train it on 38k hours of public aud",
      "published": "2026-02-25T10:47:20Z",
      "category": "cs.SD",
      "link": "http://arxiv.org/abs/2602.21772v1",
      "pdf": "http://arxiv.org/pdf/2602.21772v1.pdf"
    },
    {
      "id": "http://arxiv.org/abs/2602.21741v1",
      "title": "Robust Long-Form Bangla Speech Processing: Automatic Speech Recognition and Speaker Diarization",
      "authors": [
        "MD. Sagor Chowdhury",
        "Adiba Fairooz Chowdhury"
      ],
      "summary": "We describe our end-to-end system for Bengali long-form speech recognition (ASR) and speaker diarization submitted to the DL Sprint 4.0 competition on Kaggle. Bengali presents substantial challenges for both tasks: a large phoneme inventory, significant dialectal variation, frequent code-mixing with English, and a relative scarcity of large-scale labelled corpora. For ASR we achieve a best private Word Error Rate (WER) of 0.37738 and public WER of 0.36137, combining a BengaliAI fine-tuned Whispe",
      "published": "2026-02-25T09:52:32Z",
      "category": "cs.SD",
      "link": "http://arxiv.org/abs/2602.21741v1",
      "pdf": "http://arxiv.org/pdf/2602.21741v1.pdf"
    }
  ],
  "blogs": []
}